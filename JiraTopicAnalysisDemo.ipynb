{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import necessary libraries and functions/ classes\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "##uncomment and run if wordnet is not downloaded\n",
    "##import nltk\n",
    "##nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gets the root of the word\n",
    "def lemmatize_stemming(word):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(word, pos ='v'))\n",
    "\n",
    "### applies root of a word if word \n",
    "def wordCleaner(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text.translate(str.maketrans('','',punctuation))):\n",
    "        if token not in STOPWORDS and len(token)>2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Preproccessed: \n",
      "['pool', 'busi', 'avail', 'connect', 'queue', 'reach', 'max', 'size']\n"
     ]
    }
   ],
   "source": [
    "### summary from CASSANDRA-15302 is provided\n",
    "example = \"Pool is busy (no available connection and the queue has reached its max size 256)))\"\n",
    "print('Example Preproccessed: ')\n",
    "print(wordCleaner(example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Remove hook initialization in ctor from Bigtab...\n",
      "1                             Display DAG from the CLI\n",
      "2       Add interactivity to pre-commit image building\n",
      "3                      Add typehint to GCP's Task Hook\n",
      "4    Yamllint is not needed as prerequisite for pre...\n",
      "Name: summary, dtype: object\n",
      "0                  [remov, hook, initi, ctor]\n",
      "1                         [display, dag, cli]\n",
      "2     [add, interact, precommit, imag, build]\n",
      "3           [add, typehint, gcps, task, hook]\n",
      "4    [yamllint, need, prerequisit, precommit]\n",
      "Name: summary, dtype: object\n",
      "Number of tickets : 1228 \n"
     ]
    }
   ],
   "source": [
    "#### reads the data keeps only the summary\n",
    "data = pd.read_csv('ticketData.csv')['summary']\n",
    "print(data.head())\n",
    "#### applies the preprocess function to each row\n",
    "dataProcessed = data.apply(wordCleaner)\n",
    "print(dataProcessed.head())\n",
    "print(\"Number of tickets : {} \".format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creates list of words across the documents\n",
    "#### Removes words that are too common or too rare\n",
    "dictionary = gensim.corpora.Dictionary(dataProcessed)\n",
    "dictionary.filter_extremes(no_above = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 66 (\"connect\") appears 1 time.\n",
      "Word 183 (\"size\") appears 1 time.\n",
      "Word 204 (\"pool\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "#### gets word count for each word that is present in a dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for  doc in dataProcessed]\n",
    "### Example Word\n",
    "exampleWord = bow_corpus[300]\n",
    "for i in range(len(exampleWord)):\n",
    "    print('Word {} (\\\"{}\") appears {} time.'.format(exampleWord[i][0],dictionary[exampleWord[i][0]],exampleWord[i][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.045*\"updat\" + 0.031*\"use\" + 0.030*\"fail\" + 0.021*\"add\" + 0.021*\"test\" + 0.020*\"configur\" + 0.018*\"api\" + 0.015*\"class\" + 0.015*\"hive\" + 0.015*\"implement\"\n",
      "Topic: 1 \n",
      "Words: 0.081*\"add\" + 0.040*\"support\" + 0.024*\"fail\" + 0.023*\"creat\" + 0.018*\"job\" + 0.017*\"error\" + 0.016*\"field\" + 0.015*\"java\" + 0.013*\"project\" + 0.013*\"night\"\n",
      "Topic: 2 \n",
      "Words: 0.049*\"build\" + 0.039*\"remov\" + 0.037*\"fail\" + 0.020*\"improv\" + 0.017*\"document\" + 0.017*\"option\" + 0.016*\"properti\" + 0.014*\"file\" + 0.014*\"throw\" + 0.014*\"partit\"\n",
      "Topic: 3 \n",
      "Words: 0.031*\"support\" + 0.021*\"new\" + 0.020*\"releas\" + 0.020*\"updat\" + 0.020*\"data\" + 0.019*\"check\" + 0.017*\"instal\" + 0.016*\"doc\" + 0.016*\"python\" + 0.014*\"request\"\n",
      "Topic: 4 \n",
      "Words: 0.039*\"fix\" + 0.030*\"log\" + 0.029*\"test\" + 0.024*\"upgrad\" + 0.023*\"version\" + 0.020*\"use\" + 0.020*\"spark\" + 0.018*\"file\" + 0.018*\"add\" + 0.016*\"support\"\n"
     ]
    }
   ],
   "source": [
    "### Trains a lda model and prints out words/ weights in topic\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus,num_topics= 5,id2word=dictionary, passes = 2, workers = 2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx,topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.797083854675293\t \n",
      " Topic: 0.039*\"fix\" + 0.030*\"log\" + 0.029*\"test\" + 0.024*\"upgrad\" + 0.023*\"version\"\n",
      "\n",
      "Score: 0.05195586383342743\t \n",
      " Topic: 0.031*\"support\" + 0.021*\"new\" + 0.020*\"releas\" + 0.020*\"updat\" + 0.020*\"data\"\n",
      "\n",
      "Score: 0.050492361187934875\t \n",
      " Topic: 0.049*\"build\" + 0.039*\"remov\" + 0.037*\"fail\" + 0.020*\"improv\" + 0.017*\"document\"\n",
      "\n",
      "Score: 0.05040318891406059\t \n",
      " Topic: 0.081*\"add\" + 0.040*\"support\" + 0.024*\"fail\" + 0.023*\"creat\" + 0.018*\"job\"\n",
      "\n",
      "Score: 0.05006469041109085\t \n",
      " Topic: 0.045*\"updat\" + 0.031*\"use\" + 0.030*\"fail\" + 0.021*\"add\" + 0.021*\"test\"\n"
     ]
    }
   ],
   "source": [
    "### Example Scoring to see where it would be classsified\n",
    "for index, score in sorted(lda_model[exampleWord], key = lambda x: -1*x[1]):\n",
    "    print(\"\\nScore: {}\\t \\n Topic: {}\". format(score, lda_model.print_topic(index,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7970552444458008\t \n",
      " Topic: 0.039*\"fix\" + 0.030*\"log\" + 0.029*\"test\" + 0.024*\"upgrad\" + 0.023*\"version\"\n",
      "\n",
      "Score: 0.05198429524898529\t \n",
      " Topic: 0.031*\"support\" + 0.021*\"new\" + 0.020*\"releas\" + 0.020*\"updat\" + 0.020*\"data\"\n",
      "\n",
      "Score: 0.05049246922135353\t \n",
      " Topic: 0.049*\"build\" + 0.039*\"remov\" + 0.037*\"fail\" + 0.020*\"improv\" + 0.017*\"document\"\n",
      "\n",
      "Score: 0.050403278321027756\t \n",
      " Topic: 0.081*\"add\" + 0.040*\"support\" + 0.024*\"fail\" + 0.023*\"creat\" + 0.018*\"job\"\n",
      "\n",
      "Score: 0.050064701586961746\t \n",
      " Topic: 0.045*\"updat\" + 0.031*\"use\" + 0.030*\"fail\" + 0.021*\"add\" + 0.021*\"test\"\n"
     ]
    }
   ],
   "source": [
    "### Testing on new Jira issue:\n",
    "issueSummary = 'Reorganize public v2 catalog API'\n",
    "bow_vector = dictionary.doc2bow(wordCleaner(issueSummary))\n",
    "for index, score in sorted(lda_model[exampleWord], key = lambda x: -1*x[1]):\n",
    "    print(\"\\nScore: {}\\t \\n Topic: {}\". format(score, lda_model.print_topic(index,5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
